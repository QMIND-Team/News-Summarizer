# -*- coding: utf-8 -*-
"""get_article_encodings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nAZAb1JQOPtX8zy-eX_RJofkv9OU1GtQ
"""

#Function get_article_encodings() - encodes article sentences using xlnet, xlnetlmhead, or bert
def get_article_encodings(article_sentences, encoding_tokenizer, encoding_model):
    
  #KMeans clustering
  from sklearn.cluster import KMeans
  from sklearn.metrics import pairwise_distances_argmin_min

  #Sentence tokenization
  import nltk
  from nltk.tokenize import sent_tokenize
  nltk.download('punkt')

  from tqdm import tqdm, trange
  import pandas as pd
  import torch
  import io
  import numpy as np
  import matplotlib.pyplot as plt
  
  article_encodings=[]
  for sentence in article_sentences:
    input_ids = torch.tensor(encoding_tokenizer.encode(sentence)).unsqueeze(0)  # Batch size 1
    outputs = encoding_model(input_ids)
    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
    #print(last_hidden_states.shape, type(last_hidden_states), last_hidden_states)
    mean_last_hidden_states = last_hidden_states.mean(-2)
    #print(mean_last_hidden_states.shape, type(mean_last_hidden_states), mean_last_hidden_states)
    article_encodings.append(mean_last_hidden_states.data)
  article_encodings = torch.stack(article_encodings)
  article_encodings = torch.squeeze(article_encodings, dim=1)
  print(article_encodings.shape, type(article_encodings), article_encodings)
  return article_encodings